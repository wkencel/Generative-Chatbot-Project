{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mF_9Y8FYrQCP",
    "outputId": "35f12c03-75cd-4daa-ee98-68cc224274ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1045: They do not!\n",
      "L1044: They do to!\n",
      "L985: I hope so.\n",
      "L984: She okay?\n",
      "L925: Let's go.\n",
      "L924: Wow\n",
      "L872: Okay -- you're gonna need to learn how to lie.\n",
      "L871: No\n",
      "L870: I'm kidding.  You know how sometimes you just become this \"persona\"?  And you don't know how to quit?\n",
      "L869: Like my fear of wearing pastels?\n",
      "Number of questions: 221416\n",
      "Number of answers: 221416\n",
      "Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\n",
      "Well, I thought we'd start with pronunciation, if that's okay with you.\n",
      "\n",
      "Well, I thought we'd start with pronunciation, if that's okay with you.\n",
      "Not the hacking and gagging and spitting part.  Please.\n",
      "\n",
      "Not the hacking and gagging and spitting part.  Please.\n",
      "Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\n",
      "\n",
      "You're asking me out.  That's so cute. What's your name again?\n",
      "Forget it.\n",
      "\n",
      "No, no, it's my fault -- we didn't have a proper introduction ---\n",
      "Cameron.\n",
      "\n",
      "160580\n",
      "160580\n",
      "\n",
      "Question 1: there\n",
      "Answer 1: startseq where endseq\n",
      "\n",
      "Question 2: hi\n",
      "Answer 2: startseq looks like things worked out tonight huh endseq\n",
      "\n",
      "Question 3: but\n",
      "Answer 3: startseq you always been this selfish endseq\n",
      "\n",
      "Question Vocabulary Size: 32612\n",
      "Answer Vocabulary Size: 31596\n",
      "Training set size: 128464\n",
      "Validation set size: 16058\n",
      "Test set size: 16058\n"
     ]
    }
   ],
   "source": [
    "# load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Bidirectional, Dense, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load movie line from local directory (remember to change to YOUR file location)\n",
    "with open('movie_lines.txt', 'r', encoding='utf-8', errors='replace') as file:\n",
    "    lines = pd.read_table(file, sep='\\t', header=None, on_bad_lines='skip')\n",
    "\n",
    "# load conversation file from github (no need to change anything here for data to load)\n",
    "convolines = pd.read_table('https://raw.githubusercontent.com/wkencel/Generative-Chatbot-Project/refs/heads/main/movie_conversations.txt', sep='\\t', header=None, encoding='utf-8', on_bad_lines='skip')\n",
    "\n",
    "# view lines\n",
    "lines[:10]\n",
    "\n",
    "convolines[:10]\n",
    "\n",
    "# Create dictionary to map each line's id with its text\n",
    "id2line = {}\n",
    "\n",
    "# Iterate over each row in the dataframe and access the text data\n",
    "for line in lines[0]:  # Access the first column which contains the movie lines\n",
    "    _line = line.split(' +++$+++ ')\n",
    "    if len(_line) == 5:\n",
    "        id2line[_line[0]] = _line[4]\n",
    "\n",
    "# Print the first 10 entries\n",
    "for i, (key, value) in enumerate(id2line.items()):\n",
    "    if i < 10:  # Change this number to see more or fewer entries\n",
    "        print(f\"{key}: {value}\")\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Create a list of all of the conversations' lines' ids\n",
    "convs = []\n",
    "for index, row in convolines.iterrows():\n",
    "    line = row[0]  # Access the first column of the row\n",
    "    _line = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\", \"\").replace(\" \", \"\")\n",
    "    convs.append(_line.split(','))\n",
    "\n",
    "# Print the first 10 entries\n",
    "convs[:10]\n",
    "\n",
    "# Sort the sentences: inputs (questions) and targets (answers)\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "for conv in convs:\n",
    "    for i in range(len(conv) - 1):\n",
    "        if conv[i] in id2line and conv[i + 1] in id2line:\n",
    "            questions.append(id2line[conv[i]])\n",
    "            answers.append(id2line[conv[i + 1]])\n",
    "\n",
    "print(\"Number of questions:\", len(questions))\n",
    "print(\"Number of answers:\", len(answers))\n",
    "\n",
    "# Check if data is loaded correctly\n",
    "limit = 0\n",
    "for i in range(limit, limit+5):\n",
    "    print(questions[i])\n",
    "    print(answers[i])\n",
    "    print()\n",
    "\n",
    "# Create a DataFrame from questions and answers\n",
    "data = {'Questions': questions, 'Answers': answers}\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "data.head()\n",
    "\n",
    "data.shape\n",
    "\n",
    "# Remove duplicates\n",
    "data.drop_duplicates(inplace  = True)\n",
    "\n",
    "data.shape\n",
    "\n",
    "# Function for cleaning the text: lowercase, remove punctuations, and replace certain words\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"there's\", \"there is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "data['Questions'] = data['Questions'].apply(clean_text)\n",
    "data['Answers'] = data['Answers'].apply(clean_text)\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "data.head()\n",
    "\n",
    "# More text pre-processing\n",
    "import string\n",
    "\n",
    "exclude = set(string.punctuation)\n",
    "remove_digits = str.maketrans('', '', string.digits)\n",
    "\n",
    "# More text pre-processing\n",
    "def preprocess_questions_sentences(sent):\n",
    "    '''Function to preprocess English Sentence'''\n",
    "    sent = sent.lower()\n",
    "    sent = sent.replace(\"'\", '')\n",
    "    sent = ''.join(ch for ch in sent if ch not in exclude)\n",
    "    sent = sent.translate(remove_digits)\n",
    "\n",
    "    sent = sent.strip()\n",
    "    sent = re.sub(\" +\", \" \", sent)\n",
    "    return sent\n",
    "\n",
    "\n",
    "# include SOS (start of sent.) & EOS (end of sent.) tokens\n",
    "def preprocess_answer_sentence(sent):\n",
    "    if isinstance(sent, str):\n",
    "        sent = sent.lower()\n",
    "        sent = sent.replace(\"'\", '')\n",
    "        sent = ''.join(ch for ch in sent if ch not in exclude)\n",
    "        sent = sent.translate(remove_digits)\n",
    "        sent = sent.strip()\n",
    "        sent = re.sub(\" +\", \" \", sent)\n",
    "        sent = \"startseq \" + sent + \" endseq\"\n",
    "        return sent\n",
    "    else:\n",
    "\n",
    "        return sent\n",
    "\n",
    "# Apply preprocess function on data\n",
    "data['Questions'] = data['Questions'].apply(preprocess_questions_sentences)\n",
    "data['Answers'] = data['Answers'].apply(preprocess_answer_sentence)\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "data.head()\n",
    "\n",
    "# Remove questions and answers shorter than 1 word and longer than 20 words\n",
    "min_line_length = 1\n",
    "max_line_length = 20\n",
    "\n",
    "# Create a function to count the number of words in a text\n",
    "def count_words(text):\n",
    "    return len(text.split())\n",
    "\n",
    "# Filter the DataFrame\n",
    "filtered_data = data[\n",
    "    (data['Questions'].apply(count_words).between(min_line_length, max_line_length)) &\n",
    "    (data['Answers'].apply(count_words).between(min_line_length, max_line_length))\n",
    "]\n",
    "\n",
    "# Update the original DataFrame\n",
    "data = filtered_data\n",
    "\n",
    "data.head()\n",
    "\n",
    "# Sort Qs and As by length of questions to reduce amount of padding during training\n",
    "# Hope to speed up training and reduce the loss\n",
    "\n",
    "# Convert questions and answers to their respective lengths\n",
    "data['Question_Length'] = data['Questions'].apply(lambda x: len(x.split()))\n",
    "data['Answer_Length'] = data['Answers'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Sort Qs and As by length of questions\n",
    "sorted_questions = []\n",
    "sorted_answers = []\n",
    "\n",
    "for length in range(1, max_line_length + 1):\n",
    "    for index, row in data.iterrows():\n",
    "        if row['Question_Length'] == length:\n",
    "            sorted_questions.append(row['Questions'])\n",
    "            sorted_answers.append(row['Answers'])\n",
    "\n",
    "# Output the results\n",
    "print(len(sorted_questions))\n",
    "print(len(sorted_answers))\n",
    "print()\n",
    "for i in range(min(3, len(sorted_questions))):  # Use min to avoid index errors\n",
    "    print(f\"Question {i + 1}: {sorted_questions[i]}\")\n",
    "    print(f\"Answer {i + 1}: {sorted_answers[i]}\")\n",
    "    print()\n",
    "\n",
    "# Sort the DataFrame by question length\n",
    "data = data.sort_values(by='Question_Length')\n",
    "\n",
    "# Reset index if needed\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Output the sorted DataFrame\n",
    "data[['Questions', 'Answers', 'Question_Length']].head()\n",
    "\n",
    "data.shape\n",
    "\n",
    "# Convert DataFrame columns to lists\n",
    "q_sentences = data['Questions'].tolist()\n",
    "a_sentences = data['Answers'].tolist()\n",
    "\n",
    "# Define the split ratios\n",
    "train_ratio = 0.80  # 80% for training\n",
    "val_ratio = 0.10    # 10% for validation\n",
    "test_ratio = 0.10   # 10% for testing\n",
    "\n",
    "# Ensure the sum of ratios equals 1\n",
    "assert train_ratio + val_ratio + test_ratio == 1.0, \"Split ratios must sum to 1.\"\n",
    "\n",
    "# Split into training and temporary sets (which will later be split into validation and test)\n",
    "train_q_sents, temp_q_sents, train_a_sents, temp_a_sents = train_test_split(\n",
    "    q_sentences, a_sentences, test_size=(1 - train_ratio), random_state=42, shuffle=True)\n",
    "\n",
    "# Now split the temporary set into validation and test sets\n",
    "val_size = val_ratio / (val_ratio + test_ratio)  # Calculate validation size relative to temp set\n",
    "val_q_sents, test_q_sents, val_a_sents, test_a_sents = train_test_split(\n",
    "    temp_q_sents, temp_a_sents, test_size=val_size, random_state=42, shuffle=True)\n",
    "\n",
    "# VOCABULARY\n",
    "# Filter out non-string elements from training sets\n",
    "train_q_sents = [str(sent) for sent in train_q_sents]\n",
    "train_a_sents = [str(sent) for sent in train_a_sents]\n",
    "\n",
    "# Tokenize question sentences\n",
    "ques_tokenizer = Tokenizer(oov_token='<OOV>')\n",
    "ques_tokenizer.fit_on_texts(train_q_sents)\n",
    "ques_vocab_size = len(ques_tokenizer.word_index) + 1\n",
    "\n",
    "# Tokenize answer sentences\n",
    "ans_tokenizer = Tokenizer()\n",
    "ans_tokenizer.fit_on_texts(train_a_sents)\n",
    "ans_vocab_size = len(ans_tokenizer.word_index) + 1\n",
    "\n",
    "print(f\"Question Vocabulary Size: {ques_vocab_size}\\nAnswer Vocabulary Size: {ans_vocab_size}\")\n",
    "\n",
    "max_length = 20 #Updated from 30 to 20\n",
    "\n",
    "# Convert text to sequences\n",
    "ques_sequences = ques_tokenizer.texts_to_sequences(train_q_sents)\n",
    "ans_sequences = ans_tokenizer.texts_to_sequences(train_a_sents)\n",
    "\n",
    "# Pad sequences\n",
    "source_seqs = pad_sequences(ques_sequences, maxlen=max_length, padding='post')\n",
    "target_seqs = pad_sequences(ans_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Create training dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((source_seqs, target_seqs))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=len(source_seqs)).batch(128, drop_remainder=True)\n",
    "\n",
    "# Create validation dataset\n",
    "val_sequences = ques_tokenizer.texts_to_sequences(val_q_sents)\n",
    "val_sequences = pad_sequences(val_sequences, maxlen=max_length, padding='post')\n",
    "val_target_sequences = ans_tokenizer.texts_to_sequences(val_a_sents)\n",
    "val_target_sequences = pad_sequences(val_target_sequences, maxlen=max_length, padding='post')\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_sequences, val_target_sequences))\n",
    "val_dataset = val_dataset.batch(128, drop_remainder=True)\n",
    "\n",
    "# Create test dataset\n",
    "test_sequences = ques_tokenizer.texts_to_sequences(test_q_sents)\n",
    "test_sequences = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n",
    "test_target_sequences = ans_tokenizer.texts_to_sequences(test_a_sents)\n",
    "test_target_sequences = pad_sequences(test_target_sequences, maxlen=max_length, padding='post')\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_sequences, test_target_sequences))\n",
    "test_dataset = test_dataset.batch(128, drop_remainder=True)\n",
    "\n",
    "# Print sizes of the datasets\n",
    "print(f\"Training set size: {len(train_q_sents)}\")\n",
    "print(f\"Validation set size: {len(val_q_sents)}\")\n",
    "print(f\"Test set size: {len(test_q_sents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dEduT0mRrktY",
    "outputId": "80a6133d-4847-4e78-984f-966ea2ea1ae5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.225269317626953 Accuracy 0.0\n",
      "Epoch 1 Batch 100 Loss 2.3524129390716553 Accuracy 0.06044407933950424\n",
      "Epoch 1 Batch 200 Loss 2.5202603340148926 Accuracy 0.060855261981487274\n",
      "Epoch 1 Batch 300 Loss 2.256880283355713 Accuracy 0.06455592066049576\n",
      "Epoch 1 Batch 400 Loss 1.973192572593689 Accuracy 0.07319078594446182\n",
      "Epoch 1 Batch 500 Loss 2.3228752613067627 Accuracy 0.08799342066049576\n",
      "Epoch 1 Batch 600 Loss 1.9807785749435425 Accuracy 0.09457236528396606\n",
      "Epoch 1 Batch 700 Loss 2.103070020675659 Accuracy 0.09292763471603394\n",
      "Epoch 1 Batch 800 Loss 2.033123731613159 Accuracy 0.10115131735801697\n",
      "Epoch 1 Batch 900 Loss 1.8812826871871948 Accuracy 0.09046052396297455\n",
      "Epoch 1 Batch 1000 Loss 2.061699628829956 Accuracy 0.09457236528396606\n",
      "Epoch 1 Loss 2.1843485832214355 Accuracy 0.10608552396297455\n",
      "Epoch 2 Batch 0 Loss 1.8094481229782104 Accuracy 0.09621710330247879\n",
      "Epoch 2 Batch 100 Loss 1.876987099647522 Accuracy 0.09333881735801697\n",
      "Epoch 2 Batch 200 Loss 1.8601092100143433 Accuracy 0.09457236528396606\n",
      "Epoch 2 Batch 300 Loss 2.0075576305389404 Accuracy 0.09827302396297455\n",
      "Epoch 2 Batch 400 Loss 1.8327648639678955 Accuracy 0.0949835553765297\n",
      "Epoch 2 Batch 500 Loss 1.8803812265396118 Accuracy 0.09827302396297455\n",
      "Epoch 2 Batch 600 Loss 1.8265793323516846 Accuracy 0.09868421405553818\n",
      "Epoch 2 Batch 700 Loss 1.855322241783142 Accuracy 0.10115131735801697\n",
      "Epoch 2 Batch 800 Loss 1.9927395582199097 Accuracy 0.1027960553765297\n",
      "Epoch 2 Batch 900 Loss 2.0248653888702393 Accuracy 0.1027960553765297\n",
      "Epoch 2 Batch 1000 Loss 1.8138748407363892 Accuracy 0.09786184132099152\n",
      "Epoch 2 Loss 1.8879388570785522 Accuracy 0.09909539669752121\n",
      "Epoch 3 Batch 0 Loss 1.6191412210464478 Accuracy 0.09333881735801697\n",
      "Epoch 3 Batch 100 Loss 1.8075382709503174 Accuracy 0.09539473801851273\n",
      "Epoch 3 Batch 200 Loss 1.6436651945114136 Accuracy 0.1003289446234703\n",
      "Epoch 3 Batch 300 Loss 1.8280080556869507 Accuracy 0.09950657933950424\n",
      "Epoch 3 Batch 400 Loss 1.8209885358810425 Accuracy 0.09662828594446182\n",
      "Epoch 3 Batch 500 Loss 1.7445896863937378 Accuracy 0.09416118264198303\n",
      "Epoch 3 Batch 600 Loss 1.6726826429367065 Accuracy 0.10115131735801697\n",
      "Epoch 3 Batch 700 Loss 2.0007331371307373 Accuracy 0.10115131735801697\n",
      "Epoch 3 Batch 800 Loss 1.7346872091293335 Accuracy 0.10649671405553818\n",
      "Epoch 3 Batch 900 Loss 1.7098091840744019 Accuracy 0.09539473801851273\n",
      "Epoch 3 Batch 1000 Loss 1.780877709388733 Accuracy 0.0949835553765297\n",
      "Epoch 3 Loss 1.7857338190078735 Accuracy 0.10238486528396606\n",
      "Epoch 4 Batch 0 Loss 1.7637790441513062 Accuracy 0.10238486528396606\n",
      "Epoch 4 Batch 100 Loss 1.657096266746521 Accuracy 0.10978618264198303\n",
      "Epoch 4 Batch 200 Loss 1.7603343725204468 Accuracy 0.09909539669752121\n",
      "Epoch 4 Batch 300 Loss 1.8146027326583862 Accuracy 0.10773026198148727\n",
      "Epoch 4 Batch 400 Loss 1.8598606586456299 Accuracy 0.10567434132099152\n",
      "Epoch 4 Batch 500 Loss 1.919165849685669 Accuracy 0.10608552396297455\n",
      "Epoch 4 Batch 600 Loss 1.6273113489151 Accuracy 0.11019736528396606\n",
      "Epoch 4 Batch 700 Loss 1.9805325269699097 Accuracy 0.11389802396297455\n",
      "Epoch 4 Batch 800 Loss 1.727854609489441 Accuracy 0.10402960330247879\n",
      "Epoch 4 Batch 900 Loss 1.7465256452560425 Accuracy 0.10731907933950424\n",
      "Epoch 4 Batch 1000 Loss 1.689900279045105 Accuracy 0.11430921405553818\n",
      "Epoch 4 Loss 1.7043858766555786 Accuracy 0.11019736528396606\n",
      "Epoch 5 Batch 0 Loss 1.7228775024414062 Accuracy 0.1159539446234703\n",
      "Epoch 5 Batch 100 Loss 1.587640404701233 Accuracy 0.11924342066049576\n",
      "Epoch 5 Batch 200 Loss 1.6906152963638306 Accuracy 0.11307565867900848\n",
      "Epoch 5 Batch 300 Loss 1.5279206037521362 Accuracy 0.1184210553765297\n",
      "Epoch 5 Batch 400 Loss 1.5435237884521484 Accuracy 0.11225328594446182\n",
      "Epoch 5 Batch 500 Loss 1.5629196166992188 Accuracy 0.12006578594446182\n",
      "Epoch 5 Batch 600 Loss 1.6149609088897705 Accuracy 0.1159539446234703\n",
      "Epoch 5 Batch 700 Loss 1.6337852478027344 Accuracy 0.11430921405553818\n",
      "Epoch 5 Batch 800 Loss 1.466300368309021 Accuracy 0.10690789669752121\n",
      "Epoch 5 Batch 900 Loss 1.6679397821426392 Accuracy 0.11266447603702545\n",
      "Epoch 5 Batch 1000 Loss 1.5760859251022339 Accuracy 0.10690789669752121\n",
      "Epoch 5 Loss 1.6285021305084229 Accuracy 0.1106085553765297\n",
      "Epoch 6 Batch 0 Loss 1.3970022201538086 Accuracy 0.11101973801851273\n",
      "Epoch 6 Batch 100 Loss 1.6488193273544312 Accuracy 0.12212171405553818\n",
      "Epoch 6 Batch 200 Loss 1.4411457777023315 Accuracy 0.12006578594446182\n",
      "Epoch 6 Batch 300 Loss 1.4989900588989258 Accuracy 0.12417763471603394\n",
      "Epoch 6 Batch 400 Loss 1.5781810283660889 Accuracy 0.12047697603702545\n",
      "Epoch 6 Batch 500 Loss 1.5803111791610718 Accuracy 0.12088815867900848\n",
      "Epoch 6 Batch 600 Loss 1.807101845741272 Accuracy 0.12541118264198303\n",
      "Epoch 6 Batch 700 Loss 1.5143829584121704 Accuracy 0.11430921405553818\n",
      "Epoch 6 Batch 800 Loss 1.6266025304794312 Accuracy 0.12212171405553818\n",
      "Epoch 6 Batch 900 Loss 1.5826523303985596 Accuracy 0.12417763471603394\n",
      "Epoch 6 Batch 1000 Loss 1.4989856481552124 Accuracy 0.11184210330247879\n",
      "Epoch 6 Loss 1.5544376373291016 Accuracy 0.11225328594446182\n",
      "Epoch 7 Batch 0 Loss 1.4982187747955322 Accuracy 0.13075657188892365\n",
      "Epoch 7 Batch 100 Loss 1.45294988155365 Accuracy 0.12047697603702545\n",
      "Epoch 7 Batch 200 Loss 1.550195574760437 Accuracy 0.12582236528396606\n",
      "Epoch 7 Batch 300 Loss 1.493283748626709 Accuracy 0.11143092066049576\n",
      "Epoch 7 Batch 400 Loss 1.506758451461792 Accuracy 0.12417763471603394\n",
      "Epoch 7 Batch 500 Loss 1.4285091161727905 Accuracy 0.11883223801851273\n",
      "Epoch 7 Batch 600 Loss 1.4144880771636963 Accuracy 0.12088815867900848\n",
      "Epoch 7 Batch 700 Loss 1.476723074913025 Accuracy 0.12664473056793213\n",
      "Epoch 7 Batch 800 Loss 1.4226278066635132 Accuracy 0.12664473056793213\n",
      "Epoch 7 Batch 900 Loss 1.5072475671768188 Accuracy 0.11800986528396606\n",
      "Epoch 7 Batch 1000 Loss 1.2492293119430542 Accuracy 0.11513157933950424\n",
      "Epoch 7 Loss 1.48024320602417 Accuracy 0.1340460479259491\n",
      "Epoch 8 Batch 0 Loss 1.3328640460968018 Accuracy 0.13856907188892365\n",
      "Epoch 8 Batch 100 Loss 1.4248714447021484 Accuracy 0.140625\n",
      "Epoch 8 Batch 200 Loss 1.3759886026382446 Accuracy 0.13199013471603394\n",
      "Epoch 8 Batch 300 Loss 1.5436500310897827 Accuracy 0.15583881735801697\n",
      "Epoch 8 Batch 400 Loss 1.46809983253479 Accuracy 0.12129934132099152\n",
      "Epoch 8 Batch 500 Loss 1.4301303625106812 Accuracy 0.11924342066049576\n",
      "Epoch 8 Batch 600 Loss 1.2218254804611206 Accuracy 0.1274671107530594\n",
      "Epoch 8 Batch 700 Loss 1.3927687406539917 Accuracy 0.13322368264198303\n",
      "Epoch 8 Batch 800 Loss 1.4360817670822144 Accuracy 0.12458881735801697\n",
      "Epoch 8 Batch 900 Loss 1.4710153341293335 Accuracy 0.12212171405553818\n",
      "Epoch 8 Batch 1000 Loss 1.3223251104354858 Accuracy 0.12828947603702545\n",
      "Epoch 8 Loss 1.4078974723815918 Accuracy 0.12129934132099152\n",
      "Epoch 9 Batch 0 Loss 1.4130414724349976 Accuracy 0.14679276943206787\n",
      "Epoch 9 Batch 100 Loss 1.3627158403396606 Accuracy 0.14679276943206787\n",
      "Epoch 9 Batch 200 Loss 1.4785518646240234 Accuracy 0.1393914520740509\n",
      "Epoch 9 Batch 300 Loss 1.4246939420700073 Accuracy 0.14103618264198303\n",
      "Epoch 9 Batch 400 Loss 1.2859607934951782 Accuracy 0.140625\n",
      "Epoch 9 Batch 500 Loss 1.391485571861267 Accuracy 0.13733552396297455\n",
      "Epoch 9 Batch 600 Loss 1.5257518291473389 Accuracy 0.15254934132099152\n",
      "Epoch 9 Batch 700 Loss 1.2304060459136963 Accuracy 0.1262335479259491\n",
      "Epoch 9 Batch 800 Loss 1.3260271549224854 Accuracy 0.12705592811107635\n",
      "Epoch 9 Batch 900 Loss 1.451454758644104 Accuracy 0.1352796107530594\n",
      "Epoch 9 Batch 1000 Loss 1.4221733808517456 Accuracy 0.12047697603702545\n",
      "Epoch 9 Loss 1.3389785289764404 Accuracy 0.13610197603702545\n",
      "Epoch 10 Batch 0 Loss 1.164436936378479 Accuracy 0.12993420660495758\n",
      "Epoch 10 Batch 100 Loss 1.1008572578430176 Accuracy 0.15830592811107635\n",
      "Epoch 10 Batch 200 Loss 1.2680950164794922 Accuracy 0.15707236528396606\n",
      "Epoch 10 Batch 300 Loss 1.28695809841156 Accuracy 0.15789473056793213\n",
      "Epoch 10 Batch 400 Loss 1.3434689044952393 Accuracy 0.14268092811107635\n",
      "Epoch 10 Batch 500 Loss 1.236411690711975 Accuracy 0.1574835479259491\n",
      "Epoch 10 Batch 600 Loss 1.1333684921264648 Accuracy 0.13116776943206787\n",
      "Epoch 10 Batch 700 Loss 1.336276888847351 Accuracy 0.1472039520740509\n",
      "Epoch 10 Batch 800 Loss 1.3924123048782349 Accuracy 0.15131579339504242\n",
      "Epoch 10 Batch 900 Loss 1.2062309980392456 Accuracy 0.1459703892469406\n",
      "Epoch 10 Batch 1000 Loss 1.2253494262695312 Accuracy 0.13856907188892365\n",
      "Epoch 10 Loss 1.2744005918502808 Accuracy 0.1393914520740509\n",
      "Epoch 11 Batch 0 Loss 1.2381315231323242 Accuracy 0.15953947603702545\n",
      "Epoch 11 Batch 100 Loss 1.0997514724731445 Accuracy 0.16735197603702545\n",
      "Epoch 11 Batch 200 Loss 1.2797952890396118 Accuracy 0.15789473056793213\n",
      "Epoch 11 Batch 300 Loss 1.1962473392486572 Accuracy 0.15296052396297455\n",
      "Epoch 11 Batch 400 Loss 1.3197697401046753 Accuracy 0.16488486528396606\n",
      "Epoch 11 Batch 500 Loss 1.2447867393493652 Accuracy 0.15254934132099152\n",
      "Epoch 11 Batch 600 Loss 1.2024716138839722 Accuracy 0.16200657188892365\n",
      "Epoch 11 Batch 700 Loss 1.0459779500961304 Accuracy 0.14021381735801697\n",
      "Epoch 11 Batch 800 Loss 1.3409503698349 Accuracy 0.16858552396297455\n",
      "Epoch 11 Batch 900 Loss 1.1591291427612305 Accuracy 0.1509046107530594\n",
      "Epoch 11 Batch 1000 Loss 1.2948306798934937 Accuracy 0.1484375\n",
      "Epoch 11 Loss 1.2144683599472046 Accuracy 0.1352796107530594\n",
      "Epoch 12 Batch 0 Loss 1.11855947971344 Accuracy 0.17351973056793213\n",
      "Epoch 12 Batch 100 Loss 1.0901516675949097 Accuracy 0.16817434132099152\n",
      "Epoch 12 Batch 200 Loss 1.1809009313583374 Accuracy 0.17680920660495758\n",
      "Epoch 12 Batch 300 Loss 1.215070366859436 Accuracy 0.17639802396297455\n",
      "Epoch 12 Batch 400 Loss 1.0958951711654663 Accuracy 0.16488486528396606\n",
      "Epoch 12 Batch 500 Loss 1.298354148864746 Accuracy 0.16776315867900848\n",
      "Epoch 12 Batch 600 Loss 1.1423494815826416 Accuracy 0.14925986528396606\n",
      "Epoch 12 Batch 700 Loss 1.1404564380645752 Accuracy 0.15789473056793213\n",
      "Epoch 12 Batch 800 Loss 1.3697930574417114 Accuracy 0.1743421107530594\n",
      "Epoch 12 Batch 900 Loss 1.1786869764328003 Accuracy 0.1640625\n",
      "Epoch 12 Batch 1000 Loss 1.191404104232788 Accuracy 0.1587171107530594\n",
      "Epoch 12 Loss 1.1574691534042358 Accuracy 0.16488486528396606\n",
      "Epoch 13 Batch 0 Loss 1.1772226095199585 Accuracy 0.19490131735801697\n",
      "Epoch 13 Batch 100 Loss 1.0788840055465698 Accuracy 0.1928453892469406\n",
      "Epoch 13 Batch 200 Loss 1.0008690357208252 Accuracy 0.16200657188892365\n",
      "Epoch 13 Batch 300 Loss 1.1227339506149292 Accuracy 0.1850328892469406\n",
      "Epoch 13 Batch 400 Loss 1.1166372299194336 Accuracy 0.18009868264198303\n",
      "Epoch 13 Batch 500 Loss 1.1539020538330078 Accuracy 0.19449013471603394\n",
      "Epoch 13 Batch 600 Loss 1.2583740949630737 Accuracy 0.1821546107530594\n",
      "Epoch 13 Batch 700 Loss 1.062221884727478 Accuracy 0.1706414520740509\n",
      "Epoch 13 Batch 800 Loss 1.1280171871185303 Accuracy 0.171875\n",
      "Epoch 13 Batch 900 Loss 1.1697343587875366 Accuracy 0.17557565867900848\n",
      "Epoch 13 Batch 1000 Loss 1.128318190574646 Accuracy 0.15419407188892365\n",
      "Epoch 13 Loss 1.1033549308776855 Accuracy 0.1665296107530594\n",
      "Epoch 14 Batch 0 Loss 1.139064908027649 Accuracy 0.20394736528396606\n",
      "Epoch 14 Batch 100 Loss 1.0108360052108765 Accuracy 0.17105263471603394\n",
      "Epoch 14 Batch 200 Loss 0.9437195062637329 Accuracy 0.17475329339504242\n",
      "Epoch 14 Batch 300 Loss 1.1833300590515137 Accuracy 0.18914473056793213\n",
      "Epoch 14 Batch 400 Loss 1.0283700227737427 Accuracy 0.18544407188892365\n",
      "Epoch 14 Batch 500 Loss 1.0487245321273804 Accuracy 0.18379934132099152\n",
      "Epoch 14 Batch 600 Loss 1.0230907201766968 Accuracy 0.1940789520740509\n",
      "Epoch 14 Batch 700 Loss 1.171820878982544 Accuracy 0.20024670660495758\n",
      "Epoch 14 Batch 800 Loss 1.1076868772506714 Accuracy 0.18585526943206787\n",
      "Epoch 14 Batch 900 Loss 1.0500779151916504 Accuracy 0.18708881735801697\n",
      "Epoch 14 Batch 1000 Loss 1.1303017139434814 Accuracy 0.18708881735801697\n",
      "Epoch 14 Loss 1.051912784576416 Accuracy 0.18379934132099152\n",
      "Epoch 15 Batch 0 Loss 0.967860221862793 Accuracy 0.2043585479259491\n",
      "Epoch 15 Batch 100 Loss 0.9741410613059998 Accuracy 0.2043585479259491\n",
      "Epoch 15 Batch 200 Loss 0.8772026300430298 Accuracy 0.21381579339504242\n",
      "Epoch 15 Batch 300 Loss 0.9860559701919556 Accuracy 0.1899671107530594\n",
      "Epoch 15 Batch 400 Loss 0.9607669711112976 Accuracy 0.16817434132099152\n",
      "Epoch 15 Batch 500 Loss 1.0067347288131714 Accuracy 0.1875\n",
      "Epoch 15 Batch 600 Loss 0.9858391880989075 Accuracy 0.1850328892469406\n",
      "Epoch 15 Batch 700 Loss 1.0055370330810547 Accuracy 0.1940789520740509\n",
      "Epoch 15 Batch 800 Loss 0.9774524569511414 Accuracy 0.17763157188892365\n",
      "Epoch 15 Batch 900 Loss 0.9550639986991882 Accuracy 0.18585526943206787\n",
      "Epoch 15 Batch 1000 Loss 0.9474369883537292 Accuracy 0.18256579339504242\n",
      "Epoch 15 Loss 1.0033855438232422 Accuracy 0.18133223056793213\n",
      "Encoder and Decoder models saved to ./saved_models_15epocs/encoder_model and ./saved_models_15epocs/decoder_model in SavedModel format.\n"
     ]
    }
   ],
   "source": [
    "# Define the Seq2Seq Model with Attention Mechanism\n",
    "\n",
    "data = data.sample(frac=0.5, random_state=42)  # Reduce to 50% of the original data\n",
    "\n",
    "# Import necessary modules\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "\n",
    "# Define the accuracy metric\n",
    "accuracy_metric = SparseCategoricalAccuracy()\n",
    "\n",
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, encoder_output, hidden_state):\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden_state, 1)\n",
    "        score = tf.nn.tanh(self.W1(encoder_output) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "        # Calculate attention weights\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "\n",
    "        # Weighted sum of the encoder output using the attention weights\n",
    "        context_vector = attention_weights * encoder_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "# Encoder class definition\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.LSTM(self.enc_units,\n",
    "                                 return_sequences=True,\n",
    "                                 return_state=True,\n",
    "                                 recurrent_initializer='glorot_uniform'))\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, forward_h, forward_c, backward_h, backward_c = self.lstm(x)\n",
    "        state_h = tf.keras.layers.Concatenate()([forward_h, backward_h])\n",
    "        state_c = tf.keras.layers.Concatenate()([forward_c, backward_c])\n",
    "        return output, state_h, state_c\n",
    "\n",
    "# Decoder class definition\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = tf.keras.layers.LSTM(self.dec_units * 2,\n",
    "                                         return_sequences=True,\n",
    "                                         return_state=True,\n",
    "                                         recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        self.attention = AttentionLayer(self.dec_units)\n",
    "\n",
    "    def call(self, x, enc_output, hidden, enc_state_c):\n",
    "        context_vector, attention_weights = self.attention(enc_output, hidden)\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        output, state_h, state_c = self.lstm(x, initial_state=[hidden, enc_state_c])\n",
    "        x = self.fc(output)\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "        return x, state_h, state_c, attention_weights\n",
    "\n",
    "# Training process\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Loss function\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_dim = 512\n",
    "units = 256\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Define the encoder and decoder\n",
    "encoder = Encoder(ques_vocab_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(ans_vocab_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# Checkpoint saving\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)\n",
    "\n",
    "# Training step function with accuracy tracking\n",
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden, enc_state_c):\n",
    "    loss = 0\n",
    "    accuracy_metric.reset_state()  # Reset accuracy metric for each batch\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden, enc_state_c = encoder(inp)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_state_c = enc_state_c\n",
    "        dec_input = tf.expand_dims([ans_tokenizer.word_index['startseq']] * BATCH_SIZE, 1)\n",
    "\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            predictions, dec_hidden, dec_state_c, _ = decoder(dec_input, enc_output, dec_hidden, dec_state_c)\n",
    "\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # Update accuracy metric\n",
    "            accuracy_metric.update_state(targ[:, t], predictions)\n",
    "\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss, accuracy_metric.result()\n",
    "\n",
    "# Training loop with accuracy displayed after each epoch\n",
    "EPOCHS = 15\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    enc_hidden = tf.zeros((BATCH_SIZE, units * 2))\n",
    "    enc_state_c = tf.zeros((BATCH_SIZE, units * 2))\n",
    "\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(train_dataset.take(len(train_q_sents)//BATCH_SIZE)):\n",
    "        batch_loss, batch_accuracy = train_step(inp, targ, enc_hidden, enc_state_c)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print(f'Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy()} Accuracy {batch_accuracy.numpy()}')\n",
    "\n",
    "    # Saving the model after every epoch\n",
    "    checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "\n",
    "    print(f'Epoch {epoch+1} Loss {total_loss / (len(train_q_sents)//BATCH_SIZE)} Accuracy {batch_accuracy.numpy()}')\n",
    "\n",
    "# Validation and Testing (example code can be added for evaluation if needed)\n",
    "\n",
    "# Save paths using the TensorFlow format\n",
    "encoder_save_path = './saved_models_15epocs/encoder_model'\n",
    "decoder_save_path = './saved_models_15epocs/decoder_model'\n",
    "\n",
    "# Make directories if they don't exist\n",
    "os.makedirs(os.path.dirname(encoder_save_path), exist_ok=True)\n",
    "\n",
    "# Save the encoder and decoder models using TensorFlow SavedModel format\n",
    "encoder.save(encoder_save_path, save_format=\"tf\")\n",
    "decoder.save(decoder_save_path, save_format=\"tf\")\n",
    "\n",
    "print(f\"Encoder and Decoder models saved to {encoder_save_path} and {decoder_save_path} in SavedModel format.\")\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "\n",
    "# Create zip archives of the SavedModel directories\n",
    "encoder_zip_path = shutil.make_archive(encoder_save_path, 'zip', encoder_save_path)\n",
    "decoder_zip_path = shutil.make_archive(decoder_save_path, 'zip', decoder_save_path)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
